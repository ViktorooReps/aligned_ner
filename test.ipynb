{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T19:20:09.668525Z",
     "start_time": "2024-12-06T19:19:56.759954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BatchEncoding, PreTrainedTokenizer, AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers.data import data_collator\n",
    "\n",
    "from modelling_xlm_roberta import XLMRobertaForTokenClassification\n",
    "import nervaluate\n",
    "\n",
    "from functools import partial\n",
    "import torch\n",
    "\n",
    "from typing import Iterable\n",
    "from torch import Tensor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda'\n",
    "model_dtype = torch.bfloat16\n",
    "torch.cuda.get_device_name(0)"
   ],
   "id": "164bfe6522ca666c",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'modelling_xlm_roberta'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BatchEncoding, PreTrainedTokenizer, AutoTokenizer, Trainer, TrainingArguments\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data_collator\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodelling_xlm_roberta\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m XLMRobertaForTokenClassification\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnervaluate\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfunctools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m partial\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'modelling_xlm_roberta'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Test that layer cutting works",
   "id": "bd4e31a4b6b090d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:58:10.509620Z",
     "start_time": "2024-12-06T11:58:09.828329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_test = XLMRobertaForTokenClassification.from_pretrained('facebook/xlm-v-base')\n",
    "model_test"
   ],
   "id": "50cd8944790c35f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at facebook/xlm-v-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(901629, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:58:11.036436Z",
     "start_time": "2024-12-06T11:58:10.510696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_test = XLMRobertaForTokenClassification.from_pretrained('facebook/xlm-v-base', skip_last_layer=True)\n",
    "model_test"
   ],
   "id": "45dbf4a4718b4f0a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at facebook/xlm-v-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(901629, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-10): 11 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Works! Passing `skip_last_layer=True` removes the last layer in the transformer stack (11 x XLMRobertaLayer instead of 12 x XLMRobertaLayer)",
   "id": "d91e348b9d48ece2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Train models on the downstream tagging task and evaluate the knowledge transfer to a different language\n",
    "For this we will use CoNLL 2003 corpus (`eriktks/conll2003`, 14k examples) to train the model and Afrikaans NER Corpus (`nwu-ctext/afrikaans_ner_corpus`, 9k examples) to test the model. The validation is done over CoNLL 2003, only the final scores for Afrikaans are reported."
   ],
   "id": "9af1cd678fa696d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:58:14.956707Z",
     "start_time": "2024-12-06T11:58:11.039224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = load_dataset('eriktks/conll2003', split='train')\n",
    "valid_dataset = load_dataset('eriktks/conll2003', split='validation')\n",
    "test_dataset = load_dataset('nwu-ctext/afrikaans_ner_corpus', split='train')"
   ],
   "id": "19ee1cb470c3ca97",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Make sure that the labelling scheme is identical across datasets",
   "id": "ecafcb621612d84f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:58:14.961092Z",
     "start_time": "2024-12-06T11:58:14.957665Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset.features['ner_tags']",
   "id": "668a8c6cfaac7811",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:58:14.966316Z",
     "start_time": "2024-12-06T11:58:14.962106Z"
    }
   },
   "cell_type": "code",
   "source": "valid_dataset.features['ner_tags']",
   "id": "432d9997a6a8bcf1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:58:14.970969Z",
     "start_time": "2024-12-06T11:58:14.967541Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset.features['ner_tags']",
   "id": "8c160d6214600da3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['OUT', 'B-PERS', 'I-PERS', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The names are a bit different, but otherwise the schemes are identical",
   "id": "d61fbac113d2c666"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.1 Convert word-level tags to subtoken-level tags",
   "id": "2e089cee56f48462"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:24:56.946263Z",
     "start_time": "2024-12-06T15:24:51.000638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xlm_tok = AutoTokenizer.from_pretrained('facebook/xlm-v-base')\n",
    "xlm_tok_name = 'xlm-v'\n",
    "\n",
    "xlm_tok('test <mask> test', return_offsets_mapping=True)"
   ],
   "id": "7c3f74ba028a74e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 1340, 901628, 1340, 2], 'attention_mask': [1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 4), (4, 11), (11, 16), (0, 0)]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:37:02.037675Z",
     "start_time": "2024-12-06T16:36:54.455787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for reference\n",
    "ner_tags_scheme = np.array(['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'])\n",
    "ner_tags_ext    =          [  0,       2,       2,       4,       4,       6,       6,        8,        8]\n",
    "# the ext is used when we need to split one word into multiple sub tokens\n",
    "\n",
    "def tokenize(example: dict, tokenizer: PreTrainedTokenizer, tokenizer_name: str, max_length: int = 512) -> dict:\n",
    "    ner_tags: list[int] = example['ner_tags']\n",
    "    example_words: list[str] = example['tokens']\n",
    "    text = ' '.join(example_words)\n",
    "    \n",
    "    # map words to positions in text\n",
    "    word_positions: list[int] = example.get('word_positions', [])\n",
    "    \n",
    "    if len(word_positions) != len(example_words):\n",
    "        text_iterator = 0\n",
    "        for word in example_words:\n",
    "            while text[text_iterator:text_iterator + len(word)] != word:\n",
    "                text_iterator += 1\n",
    "                assert text_iterator < len(text)\n",
    "            \n",
    "            word_positions.append(text_iterator)\n",
    "    \n",
    "    encoding: BatchEncoding = tokenizer(text, return_offsets_mapping=True, truncation=True, max_length=max_length)\n",
    "    num_sub_tokens = len(encoding.offset_mapping)\n",
    "    \n",
    "    sub_token_iterator = 0\n",
    "    sub_token_ner_tags: list[int] = []\n",
    "    for word_id, ner_tag in enumerate(ner_tags):\n",
    "        word_start = word_positions[word_id]\n",
    "        word_end = word_start + len(example_words[word_id])\n",
    "        \n",
    "        # there may be some empty space between words. the sub tokens that include this empty space receive O label\n",
    "        # we compare with the end ([1]) to ensure that 0-length tokens are labelled as O (for example <CLS>)\n",
    "        while sub_token_iterator < num_sub_tokens and  encoding.offset_mapping[sub_token_iterator][1] <= word_start:\n",
    "            if encoding.offset_mapping[sub_token_iterator][1] - encoding.offset_mapping[sub_token_iterator][0] == 0:\n",
    "                # set to -100 for special tokens like <CLS>\n",
    "                sub_token_ner_tags.append(-100)\n",
    "            else:\n",
    "                sub_token_ner_tags.append(0)  # 0 = O\n",
    "            sub_token_iterator += 1\n",
    "            \n",
    "        ext_tag = ner_tags_ext[ner_tag]\n",
    "        \n",
    "        if sub_token_iterator < num_sub_tokens:\n",
    "            # the first sub token of a word receives original label, the rest receive extended label\n",
    "            sub_token_ner_tags.append(ner_tag)\n",
    "            sub_token_iterator += 1\n",
    "        \n",
    "        # again, we need to be careful about 0-length tokens, so we compare start ([0]) with the word end\n",
    "        while sub_token_iterator < num_sub_tokens and encoding.offset_mapping[sub_token_iterator][0] < word_end:\n",
    "            \n",
    "            # there is a weird quirk with transformers tokenizers: <SEP> token has (0, 0) offset \n",
    "            #   regardless of its real position, see https://github.com/huggingface/transformers/issues/35125\n",
    "            if encoding.offset_mapping[sub_token_iterator][1] - encoding.offset_mapping[sub_token_iterator][0] == 0:\n",
    "                sub_token_ner_tags.append(-100)\n",
    "            else:\n",
    "                sub_token_ner_tags.append(ext_tag)\n",
    "                \n",
    "            sub_token_iterator += 1\n",
    "    \n",
    "    # any tokens at the end (like <SEP>) receive O tokens\n",
    "    while sub_token_iterator < num_sub_tokens:\n",
    "        sub_token_iterator += 1\n",
    "        sub_token_ner_tags.append(0)\n",
    "        \n",
    "    return {\n",
    "        'word_positions': word_positions,\n",
    "        f'{tokenizer_name}_sub_tokens': encoding.input_ids,\n",
    "        f'{tokenizer_name}_sub_token_offsets': encoding.offset_mapping,\n",
    "        f'{tokenizer_name}_sub_token_ner_tags': sub_token_ner_tags,\n",
    "        'length': len(encoding.offset_mapping)\n",
    "    }\n",
    "\n",
    "tokenize_fn = partial(tokenize, tokenizer=xlm_tok, tokenizer_name=xlm_tok_name, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn)\n",
    "valid_dataset = valid_dataset.map(tokenize_fn)\n",
    "test_dataset = test_dataset.map(tokenize_fn)"
   ],
   "id": "21f16632e08f1c6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a28dbe69f6414936b90957ace3168a78"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e6897704fb34c19986887aeedc0dbbb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/8962 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca2c6a47c8dd461f822629344f55fe29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:45:06.063687Z",
     "start_time": "2024-12-06T15:45:06.040230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for test_idx in range(25):\n",
    "    ner_tags = torch.as_tensor(train_dataset[test_idx]['xlm-v_sub_token_ner_tags'])\n",
    "    tokens = torch.as_tensor(train_dataset[test_idx]['xlm-v_sub_tokens'])\n",
    "    print('Text:', ' '.join(train_dataset[test_idx]['tokens']))\n",
    "    print('Ents:', xlm_tok.decode(tokens[ner_tags > 0]))\n",
    "    print()"
   ],
   "id": "d1bc289c17bf712c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: EU rejects German call to boycott British lamb .\n",
      "Ents: EU German British\n",
      "\n",
      "Text: Peter Blackburn\n",
      "Ents: Peter Blackburn\n",
      "\n",
      "Text: BRUSSELS 1996-08-22\n",
      "Ents: BRUSSELS\n",
      "\n",
      "Text: The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep .\n",
      "Ents: European Commission German British\n",
      "\n",
      "Text: Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .\n",
      "Ents: Germany European Union Werner Zwingmann Britain\n",
      "\n",
      "Text: \" We do n't support any such recommendation because we do n't see any grounds for it , \" the Commission 's chief spokesman Nikolaus van der Pas told a news briefing .\n",
      "Ents: Commission Nikolaus van der Pas\n",
      "\n",
      "Text: He said further scientific study was required and if it was found that action was needed it should be taken by the European Union .\n",
      "Ents: European Union\n",
      "\n",
      "Text: He said a proposal last month by EU Farm Commissioner Franz Fischler to ban sheep brains , spleens and spinal cords from the human and animal food chains was a highly specific and precautionary move to protect human health .\n",
      "Ents: EU Franz Fischler\n",
      "\n",
      "Text: Fischler proposed EU-wide measures after reports from Britain and France that under laboratory conditions sheep could contract Bovine Spongiform Encephalopathy ( BSE ) -- mad cow disease .\n",
      "Ents: Fischler EU-wide Britain France Bovine Spongiform Encephalopathy BSE\n",
      "\n",
      "Text: But Fischler agreed to review his proposal after the EU 's standing veterinary committee , mational animal health officials , questioned if such action was justified as there was only a slight risk to human health .\n",
      "Ents: Fischler EU\n",
      "\n",
      "Text: Spanish Farm Minister Loyola de Palacio had earlier accused Fischler at an EU farm ministers ' meeting of causing unjustified alarm through \" dangerous generalisation . \"\n",
      "Ents: Spanish Loyola de Palacio Fischler EU\n",
      "\n",
      "Text: .\n",
      "Ents: \n",
      "\n",
      "Text: Only France and Britain backed Fischler 's proposal .\n",
      "Ents: France Britain Fischler\n",
      "\n",
      "Text: The EU 's scientific veterinary and multidisciplinary committees are due to re-examine the issue early next month and make recommendations to the senior veterinary officials .\n",
      "Ents: EU\n",
      "\n",
      "Text: Sheep have long been known to contract scrapie , a brain-wasting disease similar to BSE which is believed to have been transferred to cattle through feed containing animal waste .\n",
      "Ents: BSE\n",
      "\n",
      "Text: British farmers denied on Thursday there was any danger to human health from their sheep , but expressed concern that German government advice to consumers to avoid British lamb might influence consumers across Europe .\n",
      "Ents: British German British Europe\n",
      "\n",
      "Text: \" What we have to be extremely careful of is how other countries are going to take Germany 's lead , \" Welsh National Farmers ' Union ( NFU ) chairman John Lloyd Jones said on BBC radio .\n",
      "Ents: Germany Welsh National Farmers ' Union NFU John Lloyd Jones BBC radio\n",
      "\n",
      "Text: Bonn has led efforts to protect public health after consumer confidence collapsed in March after a British report suggested humans could contract an illness similar to mad cow disease by eating contaminated beef .\n",
      "Ents: Bonn British\n",
      "\n",
      "Text: Germany imported 47,600 sheep from Britain last year , nearly half of total imports .\n",
      "Ents: Germany Britain\n",
      "\n",
      "Text: It brought in 4,275 tonnes of British mutton , some 10 percent of overall imports .\n",
      "Ents: British\n",
      "\n",
      "Text: Rare Hendrix song draft sells for almost $ 17,000 .\n",
      "Ents: Hendrix\n",
      "\n",
      "Text: LONDON 1996-08-22\n",
      "Ents: LONDON\n",
      "\n",
      "Text: A rare early handwritten draft of a song by U.S. guitar legend Jimi Hendrix was sold for almost $ 17,000 on Thursday at an auction of some of the late musician 's favourite possessions .\n",
      "Ents: U.S. Jimi Hendrix\n",
      "\n",
      "Text: A Florida restaurant paid 10,925 pounds ( $ 16,935 ) for the draft of \" Ai n't no telling \" , which Hendrix penned on a piece of London hotel stationery in late 1966 .\n",
      "Ents: Florida Ai n't no telling Hendrix London\n",
      "\n",
      "Text: At the end of a January 1967 concert in the English city of Nottingham he threw the sheet of paper into the audience , where it was retrieved by a fan .\n",
      "Ents: English Nottingham\n",
      "\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Looks nice!",
   "id": "9234e8f0ab28fb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:46:20.266730Z",
     "start_time": "2024-12-06T16:46:20.257049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, examples: Iterable[dict], tokenizer_name: str):\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "        \n",
    "        for example in examples:\n",
    "            input_ids.append(torch.as_tensor(example[f'{tokenizer_name}_sub_tokens']))\n",
    "            labels.append(torch.as_tensor(example[f'{tokenizer_name}_sub_token_ner_tags']))\n",
    "        \n",
    "        self.input_ids = torch.stack(input_ids)\n",
    "        self.labels = torch.stack(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.labels[idx]\n",
    "    \n",
    "\n",
    "def collate_fn(inputs: list[(Tensor, Tensor)], *, pad_token: int) -> dict:\n",
    "    all_input_ids = []\n",
    "    all_labels = []\n",
    "    for input_ids, labels in inputs:\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    input_ids = pad_sequence(all_input_ids, batch_first=True, padding_value=pad_token)\n",
    "    \n",
    "    batch_size, seq_length = input_ids.shape\n",
    "\n",
    "    # do not attend to pad and pad does not attend to anything\n",
    "    pad_mask = (input_ids != pad_token)\n",
    "    attention_mask = (pad_mask.reshape(batch_size, 1, -1) != pad_mask.reshape(batch_size, -1, 1))\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'labels': pad_sequence(all_labels, batch_first=True, padding_value=-100),\n",
    "        'attention_mask': attention_mask\n",
    "    }"
   ],
   "id": "2bbb670d6122a3f5",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:46:20.692361Z",
     "start_time": "2024-12-06T16:46:20.686669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_ner_metrics(eval_pred) -> dict:\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # flatten predictions and labels to remove padding\n",
    "    aligned_preds = []\n",
    "    aligned_labels = []\n",
    "    for preds, lbls in zip(predictions, labels):\n",
    "        for pred, lbl in zip(preds, lbls):\n",
    "            # ignore padding and special tokens (label = -100)\n",
    "            if lbl != -100:\n",
    "                aligned_preds.append(pred)\n",
    "                aligned_labels.append(lbl)\n",
    "\n",
    "    aligned_preds = ner_tags_scheme[aligned_preds]\n",
    "    aligned_labels = ner_tags_scheme[aligned_labels]\n",
    "\n",
    "    evaluator = nervaluate.Evaluator(aligned_labels, aligned_preds, tags=['PER', 'LOC', 'ORG', 'MISC'], loader='list')\n",
    "    results, results_per_tag, _, _ = evaluator.evaluate()\n",
    "\n",
    "    overall_metrics = results[\"strict\"][\"overall\"]\n",
    "    \n",
    "    per_tag_metrics = {}\n",
    "    for tag, metrics in results_per_tag.items():\n",
    "        per_tag_metrics[tag] = {\n",
    "            \"precision\": metrics[\"strict\"][\"precision\"],\n",
    "            \"recall\": metrics[\"strict\"][\"recall\"],\n",
    "            \"f1\": metrics[\"strict\"][\"f1\"]\n",
    "        }\n",
    "\n",
    "    # Return desired metrics\n",
    "    return {\n",
    "        \"overall_precision\": overall_metrics[\"precision\"],\n",
    "        \"overall_recall\": overall_metrics[\"recall\"],\n",
    "        \"overall_f1\": overall_metrics[\"f1\"],\n",
    "        \"per_type_metrics\": per_tag_metrics\n",
    "    }"
   ],
   "id": "3da5ff8b55ce26e",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2 Train a conventional model",
   "id": "c9559c9f274e9c04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:46:57.617196Z",
     "start_time": "2024-12-06T16:46:56.927860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = XLMRobertaForTokenClassification.from_pretrained('facebook/xlm-v-base').to(device, dtype=model_dtype)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir='xlm-v-base-finetuned-l12-conll03',\n",
    "        overwrite_output_dir=True,\n",
    "        eval_strategy='steps',\n",
    "        eval_delay=0.001,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=1e-5,\n",
    "        max_steps=1000,\n",
    "        lr_scheduler_type='cosine',\n",
    "        warmup_ratio=0.1,\n",
    "        logging_steps=25,\n",
    "        bf16=True,\n",
    "        eval_steps=100,\n",
    "        dataloader_num_workers=4,\n",
    "        torch_compile=True,\n",
    "        include_num_input_tokens_seen=True\n",
    "    ),\n",
    "    data_collator=partial(collate_fn, pad_token=xlm_tok.pad_token_id),\n",
    "    train_dataset=Dataset(train_dataset, tokenizer_name='xlm-v'),\n",
    "    eval_dataset=Dataset(valid_dataset, tokenizer_name='xlm-v'),\n",
    "    compute_metrics=compute_ner_metrics\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "c13273fe9f7aff09",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at facebook/xlm-v-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[75], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m XLMRobertaForTokenClassification\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfacebook/xlm-v-base\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m      3\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m----> 4\u001B[0m     args\u001B[38;5;241m=\u001B[39m\u001B[43mTrainingArguments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mxlm-v-base-finetuned-l12-conll03\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[43moverwrite_output_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msteps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_delay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m        \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m        \u001B[49m\u001B[43mper_device_eval_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr_scheduler_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcosine\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwarmup_ratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogging_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m25\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbf16\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataloader_num_workers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtorch_compile\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m        \u001B[49m\u001B[43minclude_num_input_tokens_seen\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m     22\u001B[0m     data_collator\u001B[38;5;241m=\u001B[39mpartial(collate_fn, pad_token\u001B[38;5;241m=\u001B[39mxlm_tok\u001B[38;5;241m.\u001B[39mpad_token_id),\n\u001B[1;32m     23\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mDataset(train_dataset, tokenizer_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxlm-v\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m     24\u001B[0m     eval_dataset\u001B[38;5;241m=\u001B[39mDataset(valid_dataset, tokenizer_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxlm-v\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m     25\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39mcompute_ner_metrics\n\u001B[1;32m     26\u001B[0m )\n\u001B[1;32m     27\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[0;32m<string>:134\u001B[0m, in \u001B[0;36m__init__\u001B[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001B[0m\n",
      "File \u001B[0;32m~/PycharmProjects/prompt_ner/.venv/lib/python3.11/site-packages/transformers/training_args.py:1773\u001B[0m, in \u001B[0;36mTrainingArguments.__post_init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1771\u001B[0m \u001B[38;5;66;03m# Initialize device before we proceed\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_torch_available():\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\n\u001B[1;32m   1775\u001B[0m \u001B[38;5;66;03m# Disable average tokens when using single device\u001B[39;00m\n\u001B[1;32m   1776\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maverage_tokens_across_devices:\n",
      "File \u001B[0;32m~/PycharmProjects/prompt_ner/.venv/lib/python3.11/site-packages/transformers/training_args.py:2299\u001B[0m, in \u001B[0;36mTrainingArguments.device\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2295\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2296\u001B[0m \u001B[38;5;124;03mThe device used by this process.\u001B[39;00m\n\u001B[1;32m   2297\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2298\u001B[0m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m-> 2299\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_devices\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/prompt_ner/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:60\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[0;34m(self, obj, objtype)\u001B[0m\n\u001B[1;32m     58\u001B[0m cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obj, attr, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cached \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 60\u001B[0m     cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(obj, attr, cached)\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cached\n",
      "File \u001B[0;32m~/PycharmProjects/prompt_ner/.venv/lib/python3.11/site-packages/transformers/training_args.py:2172\u001B[0m, in \u001B[0;36mTrainingArguments._setup_devices\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sagemaker_mp_enabled():\n\u001B[1;32m   2171\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available():\n\u001B[0;32m-> 2172\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m   2173\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2174\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccelerate>=\u001B[39m\u001B[38;5;132;01m{ACCELERATE_MIN_VERSION}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2175\u001B[0m         )\n\u001B[1;32m   2176\u001B[0m \u001B[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001B[39;00m\n\u001B[1;32m   2177\u001B[0m accelerator_state_kwargs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menabled\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_configured_state\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m}\n",
      "\u001B[0;31mImportError\u001B[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_results_l12 = trainer.evaluate(Dataset(test_dataset, tokenizer_name='xlm-v'))\n",
    "test_results_l12"
   ],
   "id": "d24056baa716f185"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:47:13.723893Z",
     "start_time": "2024-12-06T16:47:13.557139Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3bf13c66b8ddefe3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: transformers[torch]\r\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "90cb1610c1f20c9a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
